# ChroniclingAmerica

This repository contains python scripts for interaction with the [Chronicling America API](https://chroniclingamerica.loc.gov/about/api/) maintained by the Library of Congress. 

## Guide to Files

This repository currently contains two python scripts (.py files) described below:
- ChroniclingBatch.py: This script runs a series of searches on the Chronicling America API, downloading the results as PDFs and generating accompanying metadata files. The main input is a dictionary of "subjects," each of which will generate its own eponymous metadata file. Each dictionary key is the subject name, which will be used to name the corresponding metadata file, while the corresponding value is the list of searches which pertain to that subject. Each individual search is itself a dictionary of search parameters. The available parameters are explained (somewhat inadequately) by [this document](https://chroniclingamerica.loc.gov/search/titles/opensearch.xml). By default, the script also specifies a json return format, 50 items per page, a start page of 1, and limits results to the District of Columbia. Comment out or alter line 43 (`payload["state"] = "District of Columbia"`) to obtain results from other states/state-level areas. When the script is run, it will run each of the searches specified through the Chronicling America API, then iterate through the results. If a resulting newspaper page has not yet been seen by this run of the script, the page is saved as a PDF (named "PDF###.pdf", where ### is a PDF number that increments upwards with each save, and saved to a directory called "PDFs" which must exist in the working directory) and terms specified by the list assignment in line 109 of the code (`highlight_text = []`) are highlighted on the image. Whether the run of the code has seen the newspaper page already or not, a metadata row is created with entries "Newspaper", "Date", "Link", "PDF", and "Search" to show that a search returned this page. Newspaper pages will therefore only appear once in the PDFs directory but will appear in metadata files as many times as they were returned by the specified searches. To avoid overwriting PDFs saved by previous runs, you can set the number the PDF filenames start at (variable "i") in line 23 of the code. The metadata files are saved as .csv files to the working directory. The default, example searches and highlight terms the script is saved with relate to development, architecture, and landscape achitecture in the District of Columbia, with all but the first subject commented out. To use the script, replace the subject/search dictionaries and highlight terms list with your own and set the PDF number iterator (i) to the number you want filenames to start at. Please note that as of now, the done_items dict the script uses to avoid duplicate downloads and the PDF filename iterator are **NOT** persistent across multiple runs, so multiple runs are liable to download duplicate files and overwrite PDFs and metadata if the subject names and PDF filename iterator are not changed between runs. 
- ChroniclingBatch copy.py: This imaginatively-named script is a modification of ChroniclingBatch.py intended to extract some insights from the actual text of search results so as to better inform the searches used in ChroniclingBatch.py. It uses the same subject/search format as its predecessor, but does not save PDF versions of the search results. Instead, it retrieves the Chronicling America OCR (optical character recognition) of each newspaper page in the results, iterates through all instances of a given string in the OCR text, and creates a metadata file logging instances of unique phrases in the form "word_before_string word_containing_string word_after_string" appear. In short, it looks for a string in the search results and tells you how often different words occur around it. This script is significantly less complete and user-friendly than ChroniclingBatch.py, and using it as-is is not recommended. 

## Requirements
To run properly, these scripts require Python 3 and the following modules:
- requests
- csv
- fitz (aka PyMuPDF)
- time
- pandas (for ChroniclingBatch copy.py only)
Additionally, the working directory must contain a directory named "PDFs" for ChroniclingBatch.py to store downloaded newpaper pages. 
